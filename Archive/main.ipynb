{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random \n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 128\n",
    "INPUT_CHANNEL = 1\n",
    "OUTPUT_CHANNEL = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "NUM_GENERATE_IMAGES = 9\n",
    "NUM_TIMESTEPS = 1000\n",
    "MIXED_PRECISION = \"fp16\"\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c49aa0173b4fa89975da2ab3320ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(r\"C:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\1. DDPM\\Code\\Code_11_Pytorch\\chest_X_ray_color\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "[\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "sample_image = dataset[0][\"images\"].unsqueeze(0).to(device)\n",
    "# print(\"Input shape\", sample_image.shape)\n",
    "# print(\"Output shape\", model(sample_image, timestep=0).sample.shape)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TIMESTEPS)\n",
    "noise = torch.randn(sample_image.shape).to(device)\n",
    "timesteps = torch.LongTensor([50]).to(device)\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).cpu().numpy()[0])\n",
    "\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "loss = F.mse_loss(noise_pred, noise)\n",
    "# print(loss)\n",
    "save_dir = \"./save_model/save_images_training\"\n",
    "def sample_image_generation(model, noise_scheduler, num_generate_images, random_seed, num_timesteps, save_dir):\n",
    "    pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "    \n",
    "    images = pipeline(\n",
    "        batch_size=num_generate_images,\n",
    "        generator=torch.manual_seed(random_seed),\n",
    "        num_inference_steps=num_timesteps\n",
    "    ).images\n",
    "    \n",
    "    # Ensure save_dir exists\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # # Convert PyTorch tensors to PIL images and save\n",
    "    # for i, img_tensor in enumerate(images):\n",
    "    #     # Assuming images are in the tensor format (C, H, W) and pixel values are normalized\n",
    "    #     img = transforms.ToPILImage()(img_tensor).convert(\"RGB\")\n",
    "    #     file_path = os.path.join(save_dir, f\"generated_image_{i+1}.png\")\n",
    "    #     img.save(file_path)\n",
    "    #     print(f\"Image {i+1} saved to {file_path}\")\n",
    "\n",
    "    # Loop through the generated images and save them\n",
    "    for i, image in enumerate(images):\n",
    "        # Convert the tensor image to a PIL Image\n",
    "        image_np = image.squeeze().numpy()  # Squeeze is used to remove channel dim if it's 1; for grayscale images\n",
    "        img = Image.fromarray(np.uint8(image_np * 255), 'L')  # Convert to uint8 and create PIL Image in 'L' mode for grayscale\n",
    "        \n",
    "        # Save the image\n",
    "        img.save(f\"{save_dir}/image_{i+1}.png\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_dataloader)*NUM_EPOCHS\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=MIXED_PRECISION,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    ")\n",
    "\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in tqdm(range(NUM_EPOCHS), position=0, leave=True):\n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "        last_batch_size = len(clean_images)\n",
    "        \n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (last_batch_size,)).to(device)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        \n",
    "        with accelerator.accumulate(model):\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        train_running_loss += loss.item()\n",
    "    train_loss = train_running_loss / (idx+1)\n",
    "    \n",
    "    train_learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train Loss EPOCH: {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Train Learning Rate EPOCH: {epoch+1}: {train_learning_rate}\")\n",
    "    if epoch%10 == 0:\n",
    "        sample_image_generation(model, noise_scheduler, NUM_GENERATE_IMAGES, RANDOM_SEED, NUM_TIMESTEPS)\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print(f\"Training Time: {stop-start:.2f}s\")\n",
    "\n",
    "\n",
    "save_dir = \"./save_model/save_images_final\"\n",
    "NUM_GENERATE_IMAGES = 50\n",
    "sample_image_generation(model, noise_scheduler, NUM_GENERATE_IMAGES, RANDOM_SEED, NUM_TIMESTEPS)\n",
    "\n",
    "# Define your save path\n",
    "save_path = r\"C:\\0_Git_Iman\\All BGSU Projcts\\Z. RA & Desertation\\1. DDPM\\Code\\Code_11_Pytorch\\DDPM-Pytorch\\save_model\\checkpoint.pth\"\n",
    "\n",
    "# Collect the states\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": accelerator.unwrap_model(model).state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "    \"accelerator_state_dict\": accelerator.state_dict()\n",
    "}\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(checkpoint, save_path)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Load states into the model, optimizer, and scheduler\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "lr_scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "# Prepare them with Accelerator again if necessary\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "# Optionally, load accelerator state\n",
    "accelerator.load_state_dict(checkpoint[\"accelerator_state_dict\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
